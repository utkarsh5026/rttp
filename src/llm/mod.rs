//! LLM / AI integration — streaming inference and prompt management.
//!
//! ## Planned Features
//!
//! - OpenAI-compatible chat completion client
//! - Anthropic Claude API client
//! - Streaming response proxying (SSE → SSE)
//! - Prompt template engine
//! - Token counting utilities
//! - Local model support (via `llama.cpp` bindings)
//!
//! ## Status: PLANNED

// TODO: Implement LLM integration layer

/// Placeholder — will become the `LlmClient` type.
pub struct LlmClient;
